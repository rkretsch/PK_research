{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b83adb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import arnie\n",
    "from arnie.pk_predictors import pk_predict\n",
    "from arnie.utils import *\n",
    "from arnie.utils import _group_into_non_conflicting_bp\n",
    "import pandas as pd\n",
    "\n",
    "def get_seq(seq_filename):\n",
    "    record = SeqIO.read(seq_filename, \"fasta\")\n",
    "    return str(record.seq)\n",
    "\n",
    "def get_sliding_windows(full_seq, step, window):\n",
    "    coords = list(range(0,len(full_seq)-window+1,step))\n",
    "    windows = []\n",
    "    for i in coords:\n",
    "        new_window = full_seq[i:i+window]\n",
    "        windows.append(new_window)\n",
    "    return windows, coords\n",
    "\n",
    "def stable_helices(dotbracket):\n",
    "    bp_list = convert_dotbracket_to_bp_list(dotbracket, allow_pseudoknots=True)\n",
    "    groups = _group_into_non_conflicting_bp(bp_list)\n",
    "    for idx, pairs in enumerate(groups):\n",
    "        if idx > 0:\n",
    "            if len(pairs) == 1:\n",
    "                return False \n",
    "            \n",
    "##this function returns a list of all structures predicted for all windows in the genome            \n",
    "def get_structures(seq_windows, coords, pk_predictor):\n",
    "    struct_list = []\n",
    "    for seq,coord in zip(seq_windows,coords):\n",
    "        dotbracket = pk_predict(seq, pk_predictor)\n",
    "        if is_PK:\n",
    "            if stable_helices:\n",
    "                struct_list.append([coord, coord+window, seq, dotbracket, 'Yes', 'Yes'])\n",
    "            else:\n",
    "                struct_list.append([coord, coord+window, seq, dotbracket, 'Yes', 'No'])\n",
    "        else:\n",
    "            struct_list.append([coord, coord+window, seq, dotbracket, 'No', 'nan'])\n",
    "    df = pd.DataFrame(struct_list,columns=[\"start\",\"end\",\"sequence\", \"struct\", \"pseudoknot\", \"stable pk helix\"])\n",
    "    return df\n",
    "\n",
    "#input is list of shape values as floats with nan values numpy objects \n",
    "def normalize_shape(shape_reacs):\n",
    "    shape_reacs = np.array(shape_reacs)\n",
    "\n",
    "    # Get rid of nan values for now\n",
    "    nonan_shape_reacs = shape_reacs[~np.isnan(shape_reacs)]\n",
    "\n",
    "    # Find Filter 1: 1.5 * Inter-Quartile Range\n",
    "    sorted_shape = np.sort(nonan_shape_reacs)\n",
    "    q1 = sorted_shape[int(0.25 * len(sorted_shape))]\n",
    "    q3 = sorted_shape[int(0.75 * len(sorted_shape))]\n",
    "    iq_range = abs(q3 - q1)\n",
    "    filter1 = next(x for x, val in \\\n",
    "        enumerate(list(sorted_shape)) if val > 1.5 * iq_range)\n",
    "\n",
    "    # Find Filter 2: 95% value\n",
    "    filter2 = int(0.95 * len(sorted_shape))\n",
    "\n",
    "    # Get maximum filter value and fiter data\n",
    "    filter_cutoff = sorted_shape[max(filter1, filter2)]\n",
    "    sorted_shape = sorted_shape[sorted_shape < filter_cutoff]\n",
    "\n",
    "    # Scalefactor: Mean of top 10th percentile of values\n",
    "    top90 = sorted_shape[int(0.9 * len(sorted_shape))]\n",
    "    scalefactor = np.mean(sorted_shape[sorted_shape > top90])\n",
    "        \n",
    "    # Scale dataset\n",
    "    return shape_reacs/scalefactor\n",
    "\n",
    "# input is text file of any shape data set, output is normalized list of values in a list (some np.nan objects)\n",
    "def get_normalized_shape_data(shape_filename):\n",
    "\n",
    "    # write shape text file to list\n",
    "    shape_file = open(\"{}\".format(shape_filename), \"r\")\n",
    "    shape_data = shape_file.read()\n",
    "    shape_data_list = shape_data.split(\"\\n\")\n",
    "    shape_file.close()\n",
    "    \n",
    "    shape_nan_list = []\n",
    "    for char in shape_data_list:\n",
    "        if char == '':\n",
    "            shape_data_list.remove('')\n",
    "        elif (char == '-999') or (char == 'nan') or (char == \"NaN\"):\n",
    "            shape_nan_list.append('nan')\n",
    "        else: \n",
    "            shape_nan_list.append(float(char))\n",
    "    \n",
    "    #convert string 'nan' to np.nan\n",
    "    shape_reacs = []\n",
    "    for char in shape_nan_list:\n",
    "        if char == 'nan':\n",
    "            shape_reacs.append(np.nan)\n",
    "        else:\n",
    "            shape_reacs.append(char)\n",
    "    \n",
    "    # normalize shape data\n",
    "    normalized_shape_data = normalize_shape(shape_reacs).tolist()\n",
    "    return normalized_shape_data\n",
    "\n",
    "#add in this capability later\n",
    "#def get_pseudoknots_with_shapeknots()\n",
    "\n",
    "def viral_knots(seq_filename, shapeknots=False, shape_rankings=False, shape_data_folder=None, \n",
    "                shape_data_sets=None, pk_predictors, step, window):\n",
    "    ### args:\n",
    "        ### seq_filename - fasta file containing RNA sequence for viral genome\n",
    "        ### shapeknots - if shapeknots is one of the predictors desired to use (must include shape data)\n",
    "        ### shape_rankings - if shape reactivity values are used to score the pseudoknots (must include shape data)\n",
    "        ### shape_data_folder - folder containing csv's with shape reactivity\n",
    "        ### shape_data_sets - names of the reactivity files (no .csv necessary) \n",
    "            #(note that this is designed to work with single or multiple tracks of shape data)\n",
    "        ### pk_predictors - list of desired predictors for use: threshknots, knotty, pknots, spotrna\n",
    "        ### step - desired number of nucleotides to slide each window\n",
    "        ### window - desired size of window to divide genome into\n",
    "    \n",
    "    \n",
    "    ##first retrieve viral sequence and normalized shape data (if directed) and sort into windows\n",
    "    seq = get_seq(seq_filename)\n",
    "    RNA_seq = seq.replace(\"T\", \"U\")\n",
    "    seq_windows, coords = get_sliding_windows(RNA_seq, step=step, window=window)\n",
    "    \n",
    "    shape_sets = []\n",
    "    all_shape_windows = []\n",
    "    if shapeknots or shape_rankings:\n",
    "        for track in shape_data_sets:\n",
    "            shape_data = get_normalized_shape_data(shape_data_folder+'/'+track+'.csv')\n",
    "            shape_sets.append(shape_data)\n",
    "        \n",
    "        for track in shape_sets:\n",
    "            shape_windows, shape_coords = get_sliding_windows(track, step=step, window=window)\n",
    "            all_shape_windows.append(shape_windows)\n",
    "    \n",
    "    ##run necessary data through each predictor and sort output into individual csvs\n",
    "    ##update this function so that it saves all structures, not just pseudoknots, \n",
    "        ##but tags the structures that are pseudoknots\n",
    "    \n",
    "    dfs = []\n",
    "    for name in pk_predictors:\n",
    "        dfs.append(get_structures(seq_windows, coords, pk_predictor=name))\n",
    "    \n",
    "    ##run shapeknots - once implemented: figure out how to write to multiple nodes \n",
    "    \n",
    "    #shapeknots_dfs = []\n",
    "    #if shapeknots:\n",
    "        #shapeknots_dfs.append(run_shapeknots(...))\n",
    "    \n",
    "    ##add the dataframe for shapeknots results to the dataframe for all other predictors\n",
    "        ##so that all predictions are in the same dataframe \n",
    "        \n",
    "    ##run consensus analysis and generate scores for each individual structure\n",
    "    ##perform calculations to determine outliers and mark them \n",
    "    ##determine the average consensus for all structures in that window, with the exception of outliers\n",
    "    ##repeat this process but focusing only on base pairs involved in pseudoknots\n",
    "    \n",
    "    ##determine the shape agreement of each individual structure with each track of shape data available\n",
    "    ##determine if there are any outliers and mark them \n",
    "    ##find the average shape score for structures within a given window, with the exception of outliers\n",
    "    ##repeat this process for all separate tracks of shape data\n",
    "    ##determine if the scores from any tracks of shape data differ from each other greatly - save all individually\n",
    "    ##calculate an average score for shape agreement for each window across all tracks of data\n",
    "    ##repeat this process, but focusing only on base pairs involved in pseudoknots\n",
    "    \n",
    "    ##reorder data into a new dataframe which contains only windows that have been tagged to contain pseudoknots \n",
    "    ##each row contains the window, the sequence, a dictionary of all predicted structures(including non-pseudoknots)\n",
    "        ##the average consensus score, average shape scores, and whether or not helices are stable (have at least 2 bps)\n",
    "    \n",
    "    ##impose thresholds for consensus and shape (both for the whole structure and for the pseudoknotted base pairs only)\n",
    "    ##filter for only those with stable helices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e903a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1a2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da69d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
